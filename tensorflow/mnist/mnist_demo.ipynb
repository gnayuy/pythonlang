{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mnist demo (https://github.com/gnayuy/pythonlang/tree/master/tensorflow/mnist)\n",
    "# by Yang (gnayuy@gmail.com) 4/17/2017\n",
    "# this demo code combine the tensorflow.org tutorial and tensorflow dev summit 2017 tensorboard talk\n",
    "#\n",
    "# tensorboard --logdir=\"/tmp/mnist_demo\" --port 6006\n",
    "\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting /tmp/mnist_demo/data/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting /tmp/mnist_demo/data/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting /tmp/mnist_demo/data/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting /tmp/mnist_demo/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST Data\n",
    "LOGDIR = '/tmp/mnist_demo/'\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(train_dir=LOGDIR + 'data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Multilayer Convolutional Network\n",
    "\n",
    "def weight_variable(shape):\n",
    "  return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"W\")\n",
    "\n",
    "def bias_variable(shape):\n",
    "  return tf.Variable(tf.constant(0.1, shape=shape), name=\"B\")\n",
    "\n",
    "# Convolution and Pooling\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# conv\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = weight_variable([5, 5, size_in, size_out])\n",
    "        b = bias_variable([size_out])\n",
    "        conv = conv2d(input, w)\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return max_pool_2x2(act)\n",
    "    \n",
    "# fc\n",
    "\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = weight_variable([size_in, size_out])\n",
    "        b = bias_variable([size_out])\n",
    "        act = tf.nn.relu(tf.matmul(input, w) + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, training accuracy 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200, training accuracy 0.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300, training accuracy 0.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400, training accuracy 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, training accuracy 0.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600, training accuracy 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 700, training accuracy 0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800, training accuracy 0.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 900, training accuracy 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, training accuracy 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1100, training accuracy 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1200, training accuracy 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1300, training accuracy 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1400, training accuracy 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500, training accuracy 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1600, training accuracy 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1700, training accuracy 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1800, training accuracy 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1900, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9742\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "learning_rate = 1E-4\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Setup placeholders, and reshape the data\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "tf.summary.image('input', x_image, 3)\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "# use 2 conv\n",
    "conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "conv_out = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "\n",
    "flattened = tf.reshape(conv_out, [-1, 7 * 7 * 64])\n",
    "\n",
    "# use 2 fc\n",
    "fc1 = fc_layer(flattened, 7 * 7 * 64, 1024, \"fc1\")\n",
    "embedding_input = fc1\n",
    "embedding_size = 1024\n",
    "\n",
    "# Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "fc1_drop = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "y_conv = fc_layer(fc1_drop, 1024, 10, \"fc2\")\n",
    "\n",
    "# Train and Evaluate the Model\n",
    "with tf.name_scope(\"xent\"):\n",
    "    xent = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels=y), name=\"xent\")\n",
    "    tf.summary.scalar(\"xent\", xent)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "        \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()\n",
    "            \n",
    "embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "assignment = embedding.assign(embedding_input)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR + 'mnist demo')\n",
    "    writer.add_graph(session.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
    "    \n",
    "    for i in range(2000):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i%100 == 0:\n",
    "            [train_accuracy, s] = session.run([accuracy, summ],feed_dict={x:batch[0], y: batch[1], keep_prob: 1.0})\n",
    "            writer.add_summary(s, i)\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            if i % 500 == 0:\n",
    "                session.run(assignment, feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "                saver.save(session, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
    "        train_step.run(feed_dict={x: batch[0], y: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}